{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Abstract Crowd Count Trainer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary modules\n",
    "# Arrange by alphabetical order\n",
    "# imports first, then import as, then from imports.\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm\n",
    "from utils.callbacks import CalculateScore\n",
    "from utils.generator import CustomGenerator\n",
    "from utils.utils import gen_density_map_gaussian\n",
    "%matplotlib inline\n",
    "plt.ioff()"
   ]
  },
  {
   "source": [
    "## GPU check\n",
    "\n",
    "Before we do anything, we'll make sure that our GPU is being used otherwise it'll be sad to leave your model to train for hours only to come back and find out that your model had been training on CPU the whole time and you're only at epoch 8."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_device = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_device) == 0:\n",
    "    print(\"No GPU detected!\")\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "source": [
    "## Parameters and model\n",
    "\n",
    "| Parameter       | Type      | Comment                                                                                                         |\n",
    "|-----------------|-----------|-----------------------------------------------------------------------------------------------------------------|\n",
    "| `dataset_path`  | `string`  | Path to your dataset, i.e. `data/ShanghaiTech/part_B`.                                                          |\n",
    "| `generate_dmap` | `boolean` | If True, generates the density map of the dataset in `{dataset_path}/density-map` with filename `DMAP_{n}.npy`. |\n",
    "| `epochs`        | `int`     | Number of epochs to train for.                                                                                  |\n",
    "| `network_name`  | `string`  | Name of your network that you are training. Used while saving model.                                            |\n",
    "| `loss_name`     | `string`  | Name of your network's loss function. Use while saving model.                                                   |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path should look like that\n",
    "# of the original dataset, e.g.\n",
    "#\n",
    "# ShanghaiTech/\n",
    "# ├── part_A\n",
    "# │   ├── test_data\n",
    "# │   │   ├── ground-truth\n",
    "# │   │   └── images\n",
    "# │   └── train_data\n",
    "# │   │   ├── ground-truth\n",
    "# │   │   └── images\n",
    "# └── part_B\n",
    "#     ├── test_data\n",
    "#     │   ├── ground-truth\n",
    "#     │   └── images\n",
    "#     └── train_data\n",
    "#         ├── ground-truth\n",
    "#         └── images\n",
    "dataset_path = \"data/ShanghaiTech/part_B\"\n",
    "generate_dmap = False\n",
    "epochs = 300\n",
    "network_name = \"CSRNet\"\n",
    "loss_name = \"MSE\""
   ]
  },
  {
   "source": [
    "## Model compilation\n",
    "\n",
    "Define your model here. The model should be compiled with your preferred optimiser, loss function, etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from models.CSRNet import CSRNet\n",
    "\n",
    "optimizer = Adam(lr=1e-5)\n",
    "model = CSRNet(input_shape=(None, None, 3))\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "# model.summary()"
   ]
  },
  {
   "source": [
    "## Custom functions for generator\n",
    "\n",
    "Since people may deal with different datasets, custom input and output reading\n",
    "functions need to be written for the generator. The data reading function should\n",
    "accept just one `path` argument."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in a directory for generator\n",
    "def list_files(path):\n",
    "    return [os.path.join(path, x) for x in os.listdir(path)]\n",
    "\n",
    "# Custom data loading function for generator\n",
    "def get_input_data(path):\n",
    "    img = cv2.imread(path)\n",
    "\n",
    "    # Some pre-processing\n",
    "    img = img / 255.0\n",
    "    img[:,:,0] = (img[:,:,0]-0.485) / 0.229\n",
    "    img[:,:,1] = (img[:,:,1]-0.456) / 0.224\n",
    "    img[:,:,2] = (img[:,:,2]-0.406) / 0.225\n",
    "\n",
    "    return img\n",
    "\n",
    "def get_output_data(path):\n",
    "    return np.load(path)\n",
    "\n",
    "generator = CustomGenerator(\n",
    "    list_files(os.path.join(dataset_path, \"train_data\", \"images\")),\n",
    "    list_files(os.path.join(dataset_path, \"train_data\", \"density-map\")), \n",
    "    get_input_data, get_output_data\n",
    ")"
   ]
  },
  {
   "source": [
    "## (Optional) Generate density map\n",
    "\n",
    "This generates numpy files which store the density map value with prefix \"DMAP_\" stored in `ShanghaiTech/part_{dataset}/{train_or_test}_data/density-map"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_dmap:\n",
    "    data_folders = [dataset_path + folder for folder in ['train_data', 'test_data']]\n",
    "    \n",
    "    IMAGE_FOLDER_NAME = \"images\"\n",
    "    GT_FOLDER_NAME = \"ground-truth\"\n",
    "    DMAP_FOLDER_NAME = \"density-map\"\n",
    "\n",
    "    IMAGE_PREFIX = \"IMG\"\n",
    "    GT_PREFIX = \"GT_IMG\"\n",
    "    DMAP_PREFIX = \"DMAP\"\n",
    "\n",
    "    # Create necessary folders\n",
    "    for folder in data_folders:\n",
    "        os.makedirs(os.path.join(folder, DMAP_FOLDER_NAME), exist_ok=True)\n",
    "\n",
    "    for folder in data_folders:\n",
    "        for file_ in tqdm(os.listdir(os.path.join(folder, IMAGE_FOLDER_NAME))):\n",
    "            img_path = os.path.join(folder, IMAGE_FOLDER_NAME, file_)\n",
    "            gt_path = img_path.replace(IMAGE_PREFIX, GT_PREFIX).replace('jpg', 'mat').replace(IMAGE_FOLDER_NAME, GT_FOLDER_NAME)\n",
    "            dmap_path = img_path.replace(IMAGE_PREFIX, DMAP_PREFIX).replace('jpg', 'npy').replace(IMAGE_FOLDER_NAME, DMAP_FOLDER_NAME)\n",
    "\n",
    "            image = cv2.imread(img_path)\n",
    "            points = loadmat(gt_path)['image_info'][0, 0][0, 0][0]\n",
    "            \n",
    "            dmap = utils.gen_density_map_gaussian(image, points)\n",
    "            np.save(dmap_path, dmap)"
   ]
  },
  {
   "source": [
    "## Training\n",
    "\n",
    "Run this and go grab coffee and exercise and play games."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validating...: 400it [00:53,  7.43it/s]\n",
      "Epoch 1 - MAE: 61.760101318359375, MSE: 96.96749877929688\n",
      "400/400 [==============================] - 166s 416ms/step - loss: 2.0034e-07\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe6d4700d30>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train_gen = generator.train_generator()\n",
    "test_gen = generator.test_generator()\n",
    "\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    epochs=1,\n",
    "    verbose=1,\n",
    "    steps_per_epoch=generator.steps_per_epoch,\n",
    "    callbacks=[CalculateScore(generator)]\n",
    ")"
   ]
  }
 ]
}